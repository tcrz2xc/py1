# Ollama Local LLM (Planned)

Private, offline-first LLM runtime.

## Purpose
- Local inference
- No cloud dependency
- Private network only

## Planned Models
- LLaMA
- Mistral
- Code-focused models

## Integration
- Internal API
- Optional SearXNG synergy
- Accessible via LAN + VPN
